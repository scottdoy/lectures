<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Reinforcement Learning</title>

    <meta name="description" content="Reinforcement Learning">    

        <meta name="author" content="Scott Doyle" />
    
    <link rel="stylesheet" href="css/reset.css">
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- Custom Addons: TikzJax -->
    <link rel="stylesheet" type="text/css" href="http://tikzjax.com/v1/fonts.css">
    <script src="http://tikzjax.com/v1/tikzjax.js"></script>
    <!-- Custom Addons: pseudocode.js -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
    <!-- Set Theme -->
        <link rel="stylesheet" href="css/theme/scottdoy.css" id="theme">
    
    <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/atelier-dune-light.css">
    

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

          </head>

  <body>

  
  <div class="reveal">
    <div class="slides">
      <!-- Custom Title Section Here -->
      <section data-background="#005bbb" id="particles-js" class="level1">
        <section id="title" class="level2">
        <h1 style="color: #e4e4e4;">Reinforcement Learning</h1>
        <p>
        <h3 style="color: #e4e4e4;">Machine Learning for Biomedical Data</h2>
        </p>
        <p style="color: #e4e4e4;"><small>Scott Doyle / scottdoy@buffalo.edu</small></p>
        </section>
      </section>

      <!-- Custom TOC Section here-->
      
      <!-- Insert Body -->
      <section id="section" class="level1">
      <h1></h1>
      <section id="reinforcement-learning" class="level2">
      <h2>Reinforcement Learning</h2>
      </section>
      <section id="learning-strategies" class="level2">
      <h2>Learning Strategies</h2>
      <p>Think about how neural networks (and people!) learn to do complex tasks.</p>
      <p class="fragment">
      We use <strong>error functions</strong>: measuring how well or how poorly we can accomplish a task.
      </p>
      <p class="fragment">
      In neural nets, we try to minimize the error function through backpropagation:
      </p>
      <ul>
      <li class="fragment">
      Differentiate the error function
      </li>
      <li class="fragment">
      Update parameters (weights and biases)
      </li>
      <li class="fragment">
      Re-calculate error
      </li>
      <li class="fragment">
      Repeat
      </li>
      </ul>
      </section>
      <section id="human-learning" class="level2">
      <h2>Human Learning</h2>
      <p><strong>Positive</strong> and <strong>Negative</strong> reinforcement: learn behavior through a combination of <strong>rewards</strong> or <strong>punishments</strong>, respectively.</p>
      <p class="fragment">
      Through a process of trial-and-error, an <strong>agent</strong> (the learner) tries to maximize its reward and minimize its punishment.
      </p>
      <p class="fragment">
      In neural nets, this process is known as <strong>reinforcement learning</strong>, or RL for short.
      </p>
      </section>
      <section id="reinforcement-learning-to-a-common-task" class="level2">
      <h2>Reinforcement Learning to a Common Task</h2>
      <p>RL is interesting because it is able to learn progressively more difficult tasks with simple reward mechanisms.</p>
      <p class="fragment">
      It’s precisely the same principle that has been used to train mice to pull levers for food pellets.
      </p>
      <p class="fragment">
      Or, if you’re like me…
      </p>
      </section>
      <section id="video-games-as-environments-for-rl" class="level2">
      <h2>Video Games as Environments for RL</h2>
      <p>Video games provide a reward / punishment mecahnism for playing properly.</p>
      <p class="fragment">
      Different games operate on different levels, but even the most basic of games (e.g. Pong) has a scoring system and a win / lose effect that allows the agent (player) to know what actions to take to maximize their score.
      </p>
      <p class="fragment">
      As games get more complex, the relationship between action and reward may be more obscure…
      </p>
      </section>
      <section id="pong" class="level2">
      <h2>Pong</h2>
      <figure>
      <img src="img/pong.jpg" style="width:70.0%" alt="" /><figcaption>Block Games</figcaption>
      </figure>
      </section>
      <section id="super-mario-brothers" class="level2">
      <h2>Super Mario Brothers</h2>
      <figure>
      <img src="img/smb.png" style="width:70.0%" alt="" /><figcaption>Platformers</figcaption>
      </figure>
      </section>
      <section id="the-legend-of-zelda" class="level2">
      <h2>The Legend of Zelda</h2>
      <figure>
      <img src="img/loz.jpg" style="width:70.0%" alt="" /><figcaption>Adventure Games</figcaption>
      </figure>
      </section>
      <section id="final-fantasy-vi" class="level2">
      <h2>Final Fantasy VI</h2>
      <figure>
      <img src="img/ffvi.jpg" style="width:60.0%" alt="" /><figcaption>Role-Playing Games</figcaption>
      </figure>
      </section>
      <section id="world-of-warcraft" class="level2">
      <h2>World of Warcraft</h2>
      <figure>
      <img src="img/thesuneater.jpg" style="width:45.0%" alt="" /><figcaption>Massively Multiplayer Online Roleplaying Games</figcaption>
      </figure>
      </section>
      <section id="complexity-boiled-down" class="level2">
      <h2>Complexity, Boiled Down</h2>
      <p>Games have a finite set of possible actions (controls) that the agent can perform, in an environment described by the pixels on the screen (plus audio cues, plus force feedback…).</p>
      <p class="fragment">
      There are different goals in each, but there is some way of keeping track of what’s going on (e.g. a score counter, an experience level, a progression through the storyline) and a direction to go in.
      </p>
      <p class="fragment">
      To keep things simple, let’s use Pong as an example.
      </p>
      </section>
      <section id="pong-the-grandparent-of-gaming" class="level2">
      <h2>Pong, The Grandparent of Gaming</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/pong.jpg" style="width:100.0%" alt="" /><figcaption>Pong</figcaption>
      </figure>
      </div>
      <div>
      <ul>
      <li class="fragment">
      A small environment: 210x160x3 screen
      </li>
      <li class="fragment">
      The agent has two inputs: UP or DOWN
      </li>
      <li class="fragment">
      Reward: +1 point for passing the opponent’s paddle
      </li>
      <li class="fragment">
      Punishment: -1 score if the ball passes your own paddle
      </li>
      <li class="fragment">
      If the ball contacts a paddle, it changes velocity based on some physics rules (angle of contact and velocity of the paddle).
      </li>
      </ul>
      </div>
      </div>
      </section>
      <section id="pong-the-grandparent-of-gaming-1" class="level2">
      <h2>Pong, The Grandparent of Gaming</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/pong.jpg" style="width:100.0%" alt="" /><figcaption>Pong</figcaption>
      </figure>
      </div>
      <div>
      <p>The game state changes with each timestep, both by the rules of the game and by user actions.</p>
      <p><strong>Training goal</strong>: For a given game state, choose an action that maximizes the eventual reward.</p>
      </div>
      </div>
      </section>
      </section>
      <section id="section-1" class="level1">
      <h1></h1>
      <section id="policy-network" class="level2">
      <h2>Policy Network</h2>
      </section>
      <section id="policy-network-1" class="level2">
      <h2>Policy Network</h2>
      <p>The first thing we need to do is construct our “agent”. This will be in the form of a <strong>policy network</strong>: it describes the agent’s decision-making process.</p>
      <p class="fragment">
      For Pong, the “decision” is which action to take (go UP or DOWN).
      </p>
      <p class="fragment">
      For more complex games, the decision might be a multiclass network: LEFT, RIGHT, JUMP, ATTACK.
      </p>
      <p class="fragment">
      The policy network has 2 layers and takes in the raw image pixels ($210\times160\times3 = 100,800$) as input and produces a single probability of moving UP.
      </p>
      <p class="fragment">
      This probability distribution is <strong>sampled</strong> to get the actual move – i.e. instead of taking the softmax and always moving UP if the probability is greater than 0.5, we flip a <strong>biased</strong> coin and take the appropriate action.
      </p>
      </section>
      <section id="mathy-description" class="level2">
      <h2>Mathy Description</h2>
      <p>So we have:</p>
      <ul>
      <li class="fragment">
      The input $\mathbf{x}$, a $d=100,800$-dimensional vector.
      </li>
      <li class="fragment">
      The weights $\mathbf{W}_{1}$, a $100,800 \times H$ matrix (where $H$ is the number of hidden nodes)
      </li>
      <li class="fragment">
      The nonlinearity, in this case RELU, which thresholds the inputs
      </li>
      <li class="fragment">
      The weights $\mathbf{W}_{2}$, a $H \times 1$ matrix
      </li>
      <li class="fragment">
      A sigmoid function $\sigma$ to yield a probability
      </li>
      </ul>
      </section>
      <section id="intuitive-explanations" class="level2">
      <h2>Intuitive Explanations</h2>
      <p>The weight matrices are initialized randomly, and the sigmoid produces an output in the range of $[0, 1]$.</p>
      <p class="fragment">
      $\mathbf{W}_{1}$ (hidden layer weights) encode various game situations (location of opponent, ball, and agent) while $\mathbf{W}_{2}$ encodes the decision-making process.
      </p>
      <p class="fragment">
      We pass the network “difference frames” as inputs, so the network understands the motion of the ball from one timepoint to the next.
      </p>
      <p class="fragment">
      Now, our goal is to train the weights $\mathbf{W}_{1}$ and $\mathbf{W}_{2}$ to play Pong.
      </p>
      </section>
      <section id="credit-assignment-problem" class="level2">
      <h2>Credit Assignment Problem</h2>
      <p><img src="img/myst.jpg" style="width:60.0%" /></p>
      <p>Have you ever played a game where you had no idea what you were supposed to do?</p>
      </section>
      <section id="credit-assignment-problem-1" class="level2">
      <h2>Credit Assignment Problem</h2>
      <p>The problem we have is somewhat sparse:</p>
      <ul>
      <li class="fragment">
      How do we know that moving UP on frame 72 (represented by 100k weights) led to us receiving a point on frame 203?
      </li>
      <li class="fragment">
      What if some actions, like spastically moving the paddle up and down, appear to provide a benefit when in reality they have nothing to do with the eventual outcome?
      </li>
      </ul>
      <p class="fragment">
      This is similar to the problem facing LSTMs (far-apart inputs affecting error). To address this, we use the idea of <strong>policy gradients</strong>.
      </p>
      </section>
      </section>
      <section id="section-2" class="level1">
      <h1></h1>
      <section id="policy-gradients" class="level2">
      <h2>Policy Gradients</h2>
      </section>
      <section id="comparison-with-supervised-learning" class="level2">
      <h2>Comparison with Supervised Learning</h2>
      <p>In traditional supervised learning, we have a “true” output label that we assign to each forward pass of the network.</p>
      <p class="fragment">
      Pong Example: If we somehow “knew” that, given a particular environment (the ball is coming at us at an oblique angle) the optimal move is to go UP, then we could build an error function that measures how “wrong” each decision is and uses backpropagation to correct the parameters that led to the incorrect action.
      </p>
      </section>
      <section id="supervised-learning-scenario" class="level2">
      <h2>Supervised Learning Scenario</h2>
      <p><img src="img/supervised_learning.png" style="width:100.0%" /></p>
      </section>
      <section id="reinforcement-learning-1" class="level2">
      <h2>Reinforcement Learning</h2>
      <p>In RL, we don’t have a label for each <strong>action</strong>. Instead, we are trying to maximize <strong>eventual total reward.</strong></p>
      <p class="fragment">
      So let’s say that the output probability is 0.7, meaning that the agent is 70% likely to move UP (and it does so).
      </p>
      <p class="fragment">
      <strong>We don’t yet know if going UP is actually beneficial.</strong> So we have to hold onto that decision until the end of the game, see whether we won, and then update the parameters accordingly.
      </p>
      <p class="fragment">
      If we win, then for each input frame that led to a specific output decision, the corresponding weights are updated to make those decisions more likely in the future.
      </p>
      <p class="fragment">
      If we lose, then it does the opposite.
      </p>
      </section>
      <section id="reinforcement-learning-2" class="level2">
      <h2>Reinforcement Learning</h2>
      <p><img src="img/reinforcement_learning.png" style="width:100.0%" /></p>
      </section>
      <section id="reinforcement-learning-rollouts" class="level2">
      <h2>Reinforcement Learning: Rollouts</h2>
      <p>Playing through a single game (i.e. from the start to the acquisition of the reward / punishment) is called a <strong>rollout</strong>.</p>
      <p class="fragment">
      Again, like a human, the system plays thousands of rollouts to learn which sets of actions are most likely to lead to rewards.
      </p>
      </section>
      </section>
      <section id="section-3" class="level1">
      <h1></h1>
      <section id="training-in-rl" class="level2">
      <h2>Training in RL</h2>
      </section>
      <section id="how-to-train" class="level2">
      <h2>How to Train</h2>
      <p>First, randomly initialize $\mathbf{W}_{1}$ and $\mathbf{W}_{2}$.</p>
      <p class="fragment">
      Then play a hunred games of Pong.
      </p>
      <p class="fragment">
      Assuming each game is 200 frames, we’ve made $200\times 100 = 20,000$ decisions; for each decision, we know the parameter gradient, which tells us how to modify the parameters (depending on the eventual result).
      </p>
      <p class="fragment">
      So now we can label every decision as either good or bad, and adjust parameters accordingly. The label comes from the outcome of the game (+1 for winning, -1 for losing), so each of the input / action pairs that was taken in the winning episode becomes slightly encouraged in the future, and each of the input/action pairs in the losing episodes becomes discouraged.
      </p>
      <p class="fragment">
      Then you repeat the process with our updated policy, and do that a bajillion times.
      </p>
      </section>
      <section id="schematic-of-rl-learning" class="level2">
      <h2>Schematic of RL Learning</h2>
      <p><img src="img/rl_episodes.png" style="width:100.0%" /></p>
      </section>
      <section id="training-is-exhausting" class="level2">
      <h2>Training is Exhausting</h2>
      <p>Training is hard because of the length of the sequences and the unpredictability of action $\rightarrow$ reward.</p>
      <p class="fragment">
      Good actions do not always lead to positive eventual outcomes, but <strong>over the course of millions of games, good actions should be encouraged more than discouraged</strong>.
      </p>
      <p class="fragment">
      You also end up with a lot of “neutral” actions that legitimately do not affect the outcome, and so they won’t be overly encouraged / discouraged.
      </p>
      </section>
      <section id="discounted-rewards" class="level2">
      <h2>Discounted Rewards</h2>
      <p>We can generalize our reward feedback by saying that at timestep $t$ we should receive some reward $r_{t}$. We can choose instead to use a discounted reward:</p>
      <p>$ R_{t} = \sum_{k=0}^{\infty} \gamma^{k}r_{t+k}$</p>
      <p class="fragment">
      where $^{k}\in[0, 1]$ called a discount factor. Thus, the reward at time $t$ is a function of <strong>all successive timepoints</strong> rather than a static reward applied to each action in a rollout.
      </p>
      <p class="fragment">
      In this formulation, later rewards are less important – that is, the reward assigned to your first action should be more closely tied to the second, third, and forth action than to the hundredth action.
      </p>
      </section>
      <section id="policy-gradient-derivations" class="level2">
      <h2>Policy Gradient Derivations</h2>
      <p>Policy Gradients are _score function gradient estimators_. Consider the expected value of a scalar score function $f(x)$ (our reward function) under a probability distribution $p(x|\theta)$ (our policy network) parameterized by $\theta$:</p>
      <p>$ E_{x~p(x|\theta)}[f(x)]$</p>
      </section>
      <section id="policy-gradient-derivations-1" class="level2">
      <h2>Policy Gradient Derivations</h2>
      <p>So how should we shift the distribution through its parameters to obtain a higher reward? <strong>Gradient descent</strong>, of course:</p>
      <p>\begin{align} \nabla_{\theta}E_{x}[f(x)] &amp;= \nabla_{\theta}\sum_{x}p(x)f(x)&amp;\quad\textrm{definition of expectation}\\ &amp;=\sum_{x}\nabla_{\theta}p(x)f(x)&amp;\quad\textrm{swap sum and gradient} \\ &amp;=\sum_{x}p(x)\frac{\nabla_{\theta}p(x)}{p(x)}f(x)&amp;\quad\textrm{multiply and divide by }p(x) \\ &amp;=\sum_{x}p(x)\nabla_{\theta}\log(p(x))f(x)&amp;\quad\nabla_{\theta}\log(z)=\frac{1}{z}\nabla_{\theta}z \\ &amp;=E_{x}[f(x)\nabla_{\theta}\log(p(x))]&amp;\quad\textrm{definition of expectation}\\ \end{align}</p>
      </section>
      <section id="policy-gradient-derviation" class="level2">
      <h2>Policy Gradient Derviation</h2>
      <p>$\nabla_{\theta}E_{x}[f(x)]= E_{x}[f(x)\nabla_{\theta}\log(p(x))]$</p>
      <p>The term $\nabla_{\theta}\log(p(x))$ gives us the direction and magnitude of the gradient according to the log of our policy network output $p(x)$ (i.e. the driver of our decision-making process), given a sampled action $x$.</p>
      <p class="fragment">
      The term $f(x)$ tells us whether the action $x$ is associated with a positive or negative outcome.
      </p>
      <p class="fragment">
      So a negative outcome will <strong>reverse</strong> the gradient values – in other words, it modifies parameter values to cause <strong>the opposite of $x$</strong> to be more likely.
      </p>
      </section>
      <section id="gradient-illustration" class="level2">
      <h2>Gradient Illustration</h2>
      <p><img src="img/policygradient.png" style="width:100.0%" /></p>
      </section>
      </section>
      <section id="section-4" class="level1">
      <h1></h1>
      <section id="implementation-and-examples" class="level2">
      <h2>Implementation and Examples</h2>
      </section>
      <section id="community-to-the-rescue" class="level2">
      <h2>Community to the Rescue!</h2>
      <p>The OpenAI Gym <em><a href="https://gym.openai.com/">(link)</a></em> has been set up to provide a number of different environments to test reinforcement learning. Check out the games people have taught AI to play!</p>
      <p class="fragment">
      Karpathy posted a 130-line Python script <em><a href="https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5">(here)</a></em> that implements a 2-layer network with 200 hidden layer units which learns to play Pong fairly well after 200,000 games (8,000 episodes). You can see a video of the computer player <em><a href="https://www.youtube.com/watch?v=YOW8m2YGtRg">here</a></em>.
      </p>
      <p class="fragment">
      Atari games are pretty popular for RL due to their limited input space (usually a single, small, static screen) and possible action space (discrete, limited inputs). Games like PacMan (<a href="https://www.youtube.com/watch?v=QilHGSYbjDQ"><strong>link</strong></a>) are another example of a nice, well-defined RL scenario.
      </p>
      <p class="fragment">
      One question to ask: Just like with CNNs, what are the networks actually “learning”? Let’s look at the learned weights!
      </p>
      </section>
      <section id="learned-weights" class="level2">
      <h2>Learned Weights</h2>
      <figure>
      <img src="img/rl_weights.png" style="width:100.0%" alt="" /><figcaption>Learned Weights from $\mathbf{W}_{1}$</figcaption>
      </figure>
      </section>
      <section id="unleash-the-gamers" class="level2">
      <h2>Unleash the Gamers</h2>
      <p>In 2016, the mad scientists at Google DeepMind (<a href="https://deepmind.com"><strong>link</strong></a>) published a paper in Nature (<a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf"><strong>link</strong></a>) describing their process for teaching a reinforcement agent how to play Go, which they called AlphaGo.</p>
      <p class="fragment">
      Then, in 2017, another AI group, OpenAI (<a href="https://openai.com/"><strong>link</strong></a>), managed to train a system that could beat DOTA 2 (<a href="https://openai.com/blog/more-on-dota-2/"><strong>link</strong></a>, <a href="https://www.youtube.com/watch?v=cLC_GHZCOVQ"><strong>video 1</strong></a>, <a href="https://www.youtube.com/watch?v=yEOEqaEgu94"><strong>video 2</strong></a>).
      </p>
      <p class="fragment">
      THEN, last year, DeepMind developed an agent that plays Starcraft 2, which (of course) they called AlphaStar (<a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/"><strong>link</strong></a>, <a href="https://www.youtube.com/watch?v=DMXvkbAtHNY"><strong>video</strong></a>).
      </p>
      </section>
      </section>
      <section id="section-5" class="level1">
      <h1></h1>
      <section id="parting-words" class="level2">
      <h2>Parting Words</h2>
      </section>
      <section id="why-did-we-do-this" class="level2">
      <h2>Why Did We Do This?</h2>
      <p>Reinforcement Learning isn’t really about teaching computers to play video games (as awesome as that is).</p>
      <p class="fragment">
      The idea is to define a very general algorithm to take in a reward / punishment mechanism and develop a complex set of human-like behavior to try and maximize rewards.
      </p>
      <p class="fragment">
      Instead of a high score, what if we wanted to maximize an investment portfolio?
      </p>
      <p class="fragment">
      Or patient responses to treatment?
      </p>
      <p class="fragment">
      Or…
      </p>
      </section>
      <section id="reinforcement-learning-in-1983" class="level2">
      <h2>Reinforcement Learning… In 1983</h2>
      <figure>
      <img src="img/wargames.jpg" style="width:80.0%" alt="" /><figcaption></figcaption>
      </figure>
      </section>
      </section>
      </div>
    </div>
    <script src="js/reveal.js"></script>
    <!-- Particles scripts -->
    <script src="lib/js/particles.js"></script>
    <script src="lib/js/app.js"></script>
    <!-- Pseudocode scripts -->
    <script>
     pseudocode.renderElement(document.getElementById("hello-world-code"));
    </script>
    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
          fragments: true,
          math: {
					    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					    config: 'TeX-AMS_HTML-full'
          },

                  transition: Reveal.getQueryHash().transition || 'fade',
        
        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true },
          { src: 'plugin/math/math.js', async: true },
          { src: 'plugin/chalkboard/chalkboard.js'}
        ],
        
        // Chalkboard Plugin Settings
				chalkboard: {
					src: "plugin/chalkboard/chalkboard.json",
					toggleChalkboardButton: { left: "80px" },
					toggleNotesButton: { left: "130px" },
// 					pen:  [ 'crosshair', 'pointer' ]
//					theme: "whiteboard",
//					background: [ 'rgba(127,127,127,.1)' , 'reveal.js-plugins/chalkboard/img/whiteboard.png' ],
// 					pen:  [ 'crosshair', 'pointer' ]
//					pen: [ url('reveal.js-plugins/chalkboard/img/boardmarker.png), auto' , 'url(reveal.js-plugins/chalkboard/img/boardmarker.png), auto' ],
//				        color: [ 'rgba(0,0,255,1)', 'rgba(0,0,255,0.5)' ],
//				        draw: [ (RevealChalkboard) ?  RevealChalkboard.drawWithPen : null , (RevealChalkboard) ? RevealChalkboard.drawWithPen : null ],
				},
				keyboard: {
				    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle chalkboard when 'c' is pressed
				    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
				    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
				     8: function() { RevealChalkboard.reset() },	// reset all chalkboard data when 'BACKSPACE' is pressed
				    68: function() { RevealChalkboard.download() },	// downlad chalkboard drawing when 'd' is pressed
					  90: function() { Recorder.downloadZip(); }, 	// press 'z' to download zip containing audio files
					  84: function() { Recorder.fetchTTS(); } 	// press 't' to fetch TTS audio files
				},
      });

    </script>
  </body>
</html>

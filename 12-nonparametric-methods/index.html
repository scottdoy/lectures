<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>NONPARAMETRIC METHODS</title>

    <meta name="description" content="NONPARAMETRIC METHODS">    

        <meta name="author" content="Scott Doyle" />
    
    <link rel="stylesheet" href="css/reset.css">
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- Custom Addons: TikzJax -->
    <link rel="stylesheet" type="text/css" href="http://tikzjax.com/v1/fonts.css">
    <script src="http://tikzjax.com/v1/tikzjax.js"></script>
    <!-- Custom Addons: pseudocode.js -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
    <!-- Set Theme -->
        <link rel="stylesheet" href="css/theme/scottdoy.css" id="theme">
    
    <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/atelier-dune-light.css">
    

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

          </head>

  <body>

  
  <div class="reveal">
    <div class="slides">
      <!-- Custom Title Section Here -->
      <section data-background="#005bbb" id="particles-js" class="level1">
        <section id="title" class="level2">
        <h1 style="color: #e4e4e4;">NONPARAMETRIC METHODS</h1>
        <p>
        <h3 style="color: #e4e4e4;">Machine Learning for Biomedical Data</h2>
        </p>
        <p style="color: #e4e4e4;"><small>Scott Doyle / scottdoy@buffalo.edu</small></p>
        </section>
      </section>

      <!-- Custom TOC Section here-->
      
      <!-- Insert Body -->
      <section id="section" class="level1">
      <h1></h1>
      <section id="recap" class="level2">
      <h2>Recap</h2>
      </section>
      <section id="recap-parameter-estimation-foundations" class="level2">
      <h2>Recap: Parameter Estimation Foundations</h2>
      <p>Core assertion in model-building and parameter estimation: Values of a random variable are observed according to a known probability law.</p>
      <p class="fragment">
      “Randomness” refers to the <strong>unpredictable variations</strong> in an observed value.
      </p>
      <p class="fragment">
      These variations can occur with a specific, pre-determined likelihood – the <strong>probability law</strong>.
      </p>
      </section>
      <section id="recap-parameter-estimation-foundations-1" class="level2">
      <h2>Recap: Parameter Estimation Foundations</h2>
      <p>The probability law (i.e. distribution) is <strong>known</strong> or assumed.</p>
      <p class="fragment">
      This distribution governs the general pattern of observed variations.
      </p>
      <p class="fragment">
      We can estimate the parameters of the distribution (denoted $\boldsymbol{\theta}$).
      </p>
      </section>
      <section id="recap-relating-data-to-a-model" class="level2">
      <h2>Recap: Relating Data to a Model</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/scatter_nonlinear.svg" alt="Relating Data to a Model" style="width:100.0%" /><figcaption>Relating Data to a Model</figcaption>
      </figure>
      </div>
      <div>
      <ul>
      <li class="fragment">
      Collect samples of the random variable.
      </li>
      <li class="fragment">
      Decide on a model (typically Gaussian).
      </li>
      <li class="fragment">
      Identify the parameter set (if Gaussian, $\boldsymbol{\theta}=(\mu, \Sigma)$).
      </li>
      <li class="fragment">
      Finally, we assume that samples in $\mathcal{D}_{i}$ give no information about $\boldsymbol{\theta}_{j}$ if $i\neq j$.
      </li>
      <li class="fragment">
      Thus our task is to estimate a total of $c$ parameter sets $\boldsymbol{\theta}_{i}$, $i\in\{1,\ldots,c\}$.
      </li>
      </ul>
      </div>
      </div>
      </section>
      <section id="recap-two-approaches-to-parameter-estimation" class="level2">
      <h2>Recap: Two Approaches to Parameter Estimation</h2>
      <div class="l-double">
      <div>
      <p><strong>Maximum Likelihood Estimation (MLE)</strong></p>
      <ul>
      <li>$\boldsymbol{\theta}_{i}$ is a set of fixed, unknown quantities that we are trying to discover.</li>
      <li>The estimated values are those that maximize the probability of observing our training data.</li>
      </ul>
      </div>
      <div>
      <p><strong>Bayesian Estimation (BAY)</strong></p>
      <ul>
      <li>$\boldsymbol{\theta}_{i}$ is a set of random variables, each with a known prior distribution.</li>
      <li>Training data observations turn these priors into posterior densities.</li>
      <li>More training “sharpens” the density near the true values of the parameters – this is known as <strong>Bayesian Learning</strong></li>
      </ul>
      </div>
      </div>
      </section>
      <section id="recap-mle-vs.-bay" class="level2">
      <h2>Recap: MLE vs. BAY</h2>
      <ul>
      <li class="fragment">
      <strong>Computational Efficiency</strong>: MLE is simpler
      </li>
      <li class="fragment">
      <strong>Interpretability</strong>: MLE yields single values, BAY yields a distribution
      </li>
      <li class="fragment">
      <strong>Model Confidence</strong>: MLE relies on a good model, BAY explicitly accounts for uncertainty
      </li>
      <li class="fragment">
      <strong>Bias-Variance</strong>: MLE can correct for bias-variance, BAY handles the tradeoff through uncertainty
      </li>
      <li class="fragment">
      <strong>Overall</strong>: MLE is simpler, BAY uses more information
      </li>
      </ul>
      </section>
      <section id="recap-bay-estimation-tied-to-sample-size" class="level2">
      <h2>Recap: BAY Estimation Tied to Sample Size</h2>
      <p>Our sample-size-dependent estimation of parameters:</p>
      <p>\begin{align} \mu_{n} &amp;= \left( \frac{n\sigma_{0}^{2}}{n\sigma_{0}^{2} + \sigma^{2}} \right) \widehat{\mu}_{n} + \frac{ \sigma^{2} }{ n\sigma_{0}^{2} + \sigma^{2} } \mu_{0} \\ \sigma_{n}^{2} &amp;= \frac{ \sigma_{0}^{2} \sigma^{2} }{ n\sigma_{0}^{2} + \sigma^{2} } \end{align}</p>
      <p class="fragment">
      If $\sigma_{n}^{2}$ is our “uncertainty”, as $n \rightarrow \infty$, our uncertainty goes towards zero: More samples means less uncertainty.
      </p>
      <p class="fragment">
      If $\mu_{n}$ is our “best guess”, as $n \rightarrow \infty$, it is a linear combination of $\widehat{\mu}_{n}$ (the sample mean) and $\mu_{0}$ (our best prior guess for $\mu$).
      </p>
      <p class="fragment">
      As long as $\sigma_{0}^{2} \neq 0$, then $\mu_{n}$ approaches the sample mean as $n \rightarrow \infty$.
      </p>
      </section>
      </section>
      <section id="section-1" class="level1">
      <h1></h1>
      <section id="nonparametric-introduction" class="level2">
      <h2>Nonparametric Introduction</h2>
      </section>
      <section id="check-our-assumptions" class="level2">
      <h2>Check Our Assumptions</h2>
      <p>The core assumption for parametric techniques relies on a known parameterized probability law.</p>
      <p class="fragment">
      What if we have <strong>no idea</strong> what model governs our sample values, or if our model is a poor match for our samples?
      </p>
      <p class="fragment">
      In other words, we either have <strong>no model</strong> or a <strong>poorly fit model</strong>.
      </p>
      <p class="fragment">
      We will examine a few nonparametric techniques, where we assume essentially “arbitrary distributions” (those without a form) or unknown distributions.
      </p>
      </section>
      <section id="histogram-methods" class="level2">
      <h2>Histogram Methods</h2>
      <div class="l-double">
      <div>
      <p><img src="img/histograms.png" style="width:100.0%" /></p>
      </div>
      <div>
      <p>Recall our training: $\mathcal{D} = \{x_{1}, \ldots, x_{N}\}$.</p>
      <p class="fragment">
      Our goal is to model $p(x)$ from $\mathcal{D}$.
      </p>
      <p class="fragment">
      In a nonparametric setting, we can choose a bin size, $\Delta_{i}$, and count the number of points falling into bin $i$.
      </p>
      <p class="fragment">
      To convert into a normalized density, divide by the total number of observations $N$ and the width $\Delta_{i}$:
      </p>
      <p class="fragment">
      $p_{i} = \frac{ n_{i} }{ N\Delta_{i} } $
      </p>
      <p class="fragment">
      Hence, the model for the density is constant over each bin (typically all $\Delta_{i}$ are equal).
      </p>
      </div>
      </div>
      </section>
      <section id="histograms-by-width" class="level2">
      <h2>Histograms By Width</h2>
      <div class="l-double">
      <div>
      <p><img src="img/histogram_binsize.png" style="width:100.0%" /></p>
      </div>
      <div>
      <ul>
      <li>Green: True density (Gaussians)</li>
      <li>Purple: Randomly-observed values</li>
      </ul>
      <p class="fragment">
      Choosing a bin width that is too small (top) or too large (bottom) give inaccurate representations.
      </p>
      <p class="fragment">
      <strong>It’s hard to select $\Delta$ properly.</strong>
      </p>
      <p class="fragment">
      Remember: $N$ is often small, and we don’t know the real distribution.
      </p>
      </div>
      </div>
      </section>
      <section id="histogram-methods-1" class="level2">
      <h2>Histogram Methods</h2>
      <p>We can continuously estimate $p(x)$ as we collect training.</p>
      <p class="fragment">
      Binning introduces discontinuities at the edges of the bin, and scales poorly as the dimensionality increases.
      </p>
      <p class="fragment">
      Thus we should do the following:
      </p>
      <ul>
      <li class="fragment">
      Break the feature domain into “regions”, and consider sample observations within some “neighborhood”.
      </li>
      <li class="fragment">
      Choose a “bin width” or smoothness parameter to accurately represent the feature density.
      </li>
      </ul>
      <p class="fragment">
      The process of calculating the density across a region is known as <strong>density estimation</strong>.
      </p>
      </section>
      </section>
      <section id="section-2" class="level1">
      <h1></h1>
      <section id="density-estimation" class="level2">
      <h2>Density Estimation</h2>
      </section>
      <section id="general-density-estimation" class="level2">
      <h2>General Density Estimation</h2>
      <p class="fragment">
      Just as we did with the histograms, we assume that our sample values fall within some region $\mathcal{R}$; the probability that a point falls within $\mathcal{R}$ is:
      </p>
      <p class="fragment">
      $ P = \int_{\mathcal{R}} p(\mathbf{x}^{\prime}) d\mathbf{x}^{\prime} $
      </p>
      <p class="fragment">
      If we observe $n$ samples, then the probability that $k$ fall within $\mathcal{R}$ is given by the <strong>binomial distribution</strong>:
      </p>
      <p class="fragment">
      $ P_{k} = {n \choose k} P^{k} (1 - P)^{n - k}, \qquad {n \choose k} = \frac{ n! }{ k!(n - k)!} $
      </p>
      <p class="fragment">
      And the expected value of $k$ is thus:
      </p>
      <p class="fragment">
      $ \mathcal{E}[k] = nP $
      </p>
      </section>
      <section id="general-density-estimation-1" class="level2">
      <h2>General Density Estimation</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/binomial_distribution.png" alt="Binomial Distribution" style="width:100.0%" /><figcaption>Binomial Distribution</figcaption>
      </figure>
      </div>
      <div>
      <p>$P_{k}$ peaks sharply around the mean, so we expect that $k/n$ is a pretty good estimate of $P$.</p>
      <p class="fragment">
      This estimate gets more and more accurate as $n$ increases (i.e. as we observe more samples).
      </p>
      </div>
      </div>
      </section>
      <section id="general-density-estimation-2" class="level2">
      <h2>General Density Estimation</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/relative_probability.svg" alt="Relative Probability" style="width:100.0%" /><figcaption>Relative Probability</figcaption>
      </figure>
      </div>
      <div>
      <p>If we assume that $p(\mathbf{x})$ is continuous and $\mathcal{R}$ is really small so $p(\mathbf{x})$ does not vary appreciably within it, then we have:</p>
      <p class="fragment">
      $ \int_{\mathcal{R}} p(\mathbf{x}^{\prime}) d\mathbf{x}^{\prime} \simeq p(\mathbf{x})V $
      </p>
      <p class="fragment">
      $V$ is the volume enclosed by $\mathcal{R}$.
      </p>
      </div>
      </div>
      </section>
      <section id="pratical-concerns" class="level2">
      <h2>Pratical Concerns</h2>
      <p>We combine the previous equations to get:</p>
      <p>$ p(\mathbf{x}) \simeq \frac{ k/n }{ V } $</p>
      <p class="fragment">
      However, this leaves some problems.
      </p>
      <p class="fragment">
      We said that $\mathcal{R}$ is small, so $p(\mathbf{x})$ doesn’t vary, but it should be large enough that the number of points falling inside it yields a sharply peaked binomial.
      </p>
      <p class="fragment">
      We can fix the volume $V$ and increase the number of training samples, but this will only yield an estimate of the space-averaged density $P/V$ (we want the actual $P$).
      </p>
      <p class="fragment">
      Also, since $V$ is the space enclosed by $\mathcal{R}$, if it is TOO small, then no points will fall inside it and we’ll end up with $p(\mathbf{x}) \simeq 0$.
      </p>
      </section>
      <section id="getting-around-our-limitations" class="level2">
      <h2>Getting Around our Limitations</h2>
      <p>So let’s start by assuming we have an infinite number of samples.</p>
      <p class="fragment">
      To estimate the density at $\mathbf{x}$, first form a sequence of regions $\mathcal{R}_{1}, \mathcal{R}_{2}, \ldots$ where $\mathcal{R}_{1}$ contains 1 sample, $\mathcal{R}_{2}$ contains two, and so on.
      </p>
      <p class="fragment">
      We can write $V_{n}$ as the volume of $\mathcal{R}_{n}$, and $k_{n}$ is the number of samples falling in $\mathcal{R}_{n}$:
      </p>
      <p class="fragment">
      $ p_{n}(\mathbf{x}) = \frac{ k_{n} / n }{ V_{n} } = \frac{ k_{n} }{ nV_{n} } $
      </p>
      <p class="fragment">
      As $n \rightarrow \infty$, we want $p_{n}(\mathbf{x})$ (the density dependent on our sample size) to converge to $p(\mathbf{x})$ (the true density around $\mathbf{x}$).
      </p>
      <p class="fragment">
      For this to happen, we must satisfy a few conditions as $n \rightarrow \infty$.
      </p>
      </section>
      <section id="convergence-requirements" class="level2">
      <h2>Convergence Requirements</h2>
      <p>$ p_{n}(\mathbf{x}) = \frac{ k_{n} }{ nV_{n} } $</p>
      <p>Here are our conditions:</p>
      <ul>
      <li class="fragment">
      $\lim_{n\rightarrow\infty} V_{n} = 0$ ensures the space-averaged $P/V$ will converge to $p(\mathbf{x})$.
      </li>
      <li class="fragment">
      $\lim_{n\rightarrow\infty} k_{n} = \infty$ ensures that the frequency ratio will converge to $P$, e.g. that the binomial distribution will be sufficiently peaked.
      </li>
      <li class="fragment">
      $\lim_{n\rightarrow\infty} k_{n} / n = 0$ is required for $p_{n}(\mathbf{x})$ to converge at all; by specifying that as $n\rightarrow\infty$, the region $\mathcal{R}_{n}$ will get a large amount of samples, but they form a very small fraction of the overall number of samples.
      </li>
      </ul>
      </section>
      <section id="two-methods-for-convergence" class="level2">
      <h2>Two Methods for Convergence</h2>
      <p>So how do we build a region that specifies these conditions?</p>
      <p class="fragment">
      <strong>Parzen Windows</strong>: Shrink a region by specifying volume $V_{n}$ as a function of $n$, such as $V_{n} = 1/\sqrt{n}$. Then, we’ll show that $p_{n}(\mathbf{x})$ converges to $p(\mathbf{x})$.
      </p>
      <p class="fragment">
      <strong>K-Nearest Neighbor</strong>: Specify $k_{n}$ as a function of $n$, such as $k_{n} = \sqrt{n}$. Then, the volume $V_{n}$ is grown until it encloses $k_{n}$ neighbors of $\mathbf{x}$.
      </p>
      <p class="fragment">
      Both of these methods converge (of course, as $n\rightarrow\infty$).
      </p>
      </section>
      <section id="two-methods-for-convergence-1" class="level2">
      <h2>Two Methods for Convergence</h2>
      <figure>
      <img src="img/convergence_both.svg" alt="Convergence Methods" style="width:100.0%" /><figcaption>Convergence Methods</figcaption>
      </figure>
      </section>
      </section>
      <section id="section-3" class="level1">
      <h1></h1>
      <section id="parzen-windows" class="level2">
      <h2>Parzen Windows</h2>
      </section>
      <section id="parzen-windows-1" class="level2">
      <h2>Parzen Windows</h2>
      <p>Let’s start by assuming the region $\mathcal{R}$ is a $d$-dimensional hypercube, with $h_{n}$ being the length of an edge. Thus the volume is given by:</p>
      <p>$ V_{n} = h_{n}^{d} $</p>
      <p class="fragment">
      We can obtain an analytic expression for $k_{n}$, the number of samples falling in the hypercube, by defining a <strong>window function</strong>:
      </p>
      <p class="fragment">
      $ \varphi(\mathbf{u}) = \begin{cases} 1 &amp; \quad |u_{j}| \leq 1/2 \quad \forall j \\ 0 &amp; \quad \text{otherwise} \end{cases} $
      </p>
      <p class="fragment">
      This function defines a unit hypercube centered at the origin.
      </p>
      </section>
      <section id="window-function" class="level2">
      <h2>Window Function</h2>
      <p>$ \varphi(\mathbf{u}) = \begin{cases} 1 &amp; \quad |u_{j}| \leq 1/2 \quad \forall j \\ 0 &amp; \quad \text{otherwise} \end{cases} $</p>
      <p class="fragment">
      We can see that if $\mathbf{u} = (\mathbf{x} - \mathbf{x}_{i})/h_{n}$, then $\varphi(\mathbf{u})$ is equal to 1 if $\mathbf{x}_{i}$ falls within the hypercube of volume $V_{n}$ centered at $\mathbf{x}$, and is zero otherwise.
      </p>
      <p class="fragment">
      The number of samples in the hypercube is given by:
      </p>
      <p class="fragment">
      $ k_{n} = \sum_{i=1}^{n} \varphi\left(\frac{\mathbf{x} - \mathbf{x}_{i}}{ h_{n} } \right) $
      </p>
      <p class="fragment">
      Recall that $p_{n}(\mathbf{x}) = \frac{ k_{n} }{ nV_{n} }$, so we get:
      </p>
      <p class="fragment">
      $ p_{n}(\mathbf{x}) = \frac{ 1 }{ n } \sum_{i=1}^{n} \frac{ 1 }{ V_{n} } \varphi \left( \frac{ \mathbf{x} - \mathbf{x}_{i} }{ h_{n} } \right) $
      </p>
      </section>
      <section id="probability-estimate" class="level2">
      <h2>Probability Estimate</h2>
      <p>$ p_{n}(\mathbf{x}) = \frac{ 1 }{ n } \sum_{i=1}^{n} \frac{ 1 }{ V_{n} } \varphi \left( \frac{ \mathbf{x} - \mathbf{x}_{i} }{ h_{n} } \right) $</p>
      <p class="fragment">
      We can make $\varphi$ – the <strong>Parzen window</strong> function – to be any function of $\mathbf{x}$, controlling how to weight samples in $\mathcal{D}$ to determine $p(\mathbf{x})$ at a specific $\mathbf{x}$.
      </p>
      <p class="fragment">
      The equation for $p_{n}(\mathbf{x})$ is an <strong>average of functions</strong> of $\mathbf{x}$ and $\mathbf{x}_{i}$.
      </p>
      <p class="fragment">
      Thus the <strong>window function</strong> is being used for <strong>interpolation</strong>.
      </p>
      <p class="fragment">
      We can ensure the density is “regular” (non-negative, sums to 1) by the conditions:
      </p>
      <p class="fragment">
      $ \varphi(\mathbf{x}) \geq 0 $
      </p>
      <p class="fragment">
      $ \int \varphi(\mathbf{u}) d\mathbf{u} = 1 $
      </p>
      </section>
      <section id="effect-of-window-width" class="level2">
      <h2>Effect of Window Width</h2>
      <p>As before, we must consider the effect of the window width $h_{n}$ on $p_{n}(\mathbf{x})$.</p>
      <p class="fragment">
      Define the function:
      </p>
      <p class="fragment">
      $ \delta_{n}(\mathbf{x}) = \frac{ 1 }{ V_{n} } \varphi\left(\frac{ \mathbf{x} }{ h_{n} } \right) $
      </p>
      <p class="fragment">
      Substituting this into our equation for the density, we get:
      </p>
      <p class="fragment">
      $ p_{n}(\mathbf{x}) = \frac{ 1 }{ n } \sum_{i=1}^{n} \delta_{n}( \mathbf{x} - \mathbf{x}_{i} ) $
      </p>
      <p class="fragment">
      Since $V_{n} = h_{n}^{d}$, the width $h_{n}$ clearly affects both the amplitude and the width of $\delta_{n}(\mathbf{x})$.
      </p>
      </section>
      <section id="effect-of-window-width-1" class="level2">
      <h2>Effect of Window Width</h2>
      <figure>
      <img src="img/histwidth.svg" alt="Window Width Visualization" style="width:100.0%" /><figcaption>Window Width Visualization</figcaption>
      </figure>
      </section>
      <section id="effect-of-window-width-2" class="level2">
      <h2>Effect of Window Width</h2>
      <figure>
      <img src="img/histwidth_pts.svg" alt="Bin size on Points" style="width:100.0%" /><figcaption>Bin size on Points</figcaption>
      </figure>
      </section>
      <section id="effect-of-window-width-3" class="level2">
      <h2>Effect of Window Width</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/hist_wide.svg" alt="Wide Histogram" style="width:70.0%" /><figcaption>Wide Histogram</figcaption>
      </figure>
      </div>
      <div>
      <p>The relationship between $h_{n}$ and the amplitude of $\delta_{n}$ is inverse.</p>
      <p class="fragment">
      If $h_{n}$ is large, then the amplitude of $\delta_{n}$ is small, and $\mathbf{x}$ must be far from $\mathbf{x}_{i}$ before $\delta_{n}( \mathbf{x} - \mathbf{x}_{i})$ deviates from $\delta_{n}(\mathbf{0})$.
      </p>
      <p class="fragment">
      If $h_{n}$ is small, then the peak value of $\delta_{n}(\mathbf{x} - \mathbf{x}_{i})$ is large and occurs close to $\mathbf{x} = \mathbf{x}_{i}$ (i.e. $\delta_{n}(\mathbf{0})$).
      </p>
      </div>
      </div>
      </section>
      <section id="effect-of-window-width-4" class="level2">
      <h2>Effect of Window Width</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/hist_thin.svg" alt="Thin Histogram" style="width:70.0%" /><figcaption>Thin Histogram</figcaption>
      </figure>
      </div>
      <div>
      <p>The relationship between $h_{n}$ and the amplitude of $\delta_{n}$ is inverse.</p>
      <p class="fragment">
      If $h_{n}$ is large, then the amplitude of $\delta_{n}$ is small, and $\mathbf{x}$ must be far from $\mathbf{x}_{i}$ before $\delta_{n}( \mathbf{x} - \mathbf{x}_{i})$ deviates from $\delta_{n}(\mathbf{0})$.
      </p>
      <p class="fragment">
      If $h_{n}$ is small, then the peak value of $\delta_{n}(\mathbf{x} - \mathbf{x}_{i})$ is large and occurs close to $\mathbf{x} = \mathbf{x}_{i}$ (i.e. $\delta_{n}(\mathbf{0})$).
      </p>
      </div>
      </div>
      </section>
      <section id="effect-of-window-width-5" class="level2">
      <h2>Effect of Window Width</h2>
      <p>For any value of $h_{n}$, the distribution is normalized:</p>
      <p class="fragment">
      $ \int \delta_{n}(\mathbf{x} - \mathbf{x}_{i}) d\mathbf{x} = \int \frac{ 1 }{ V_{n} } \varphi\left( \frac{ \mathbf{x} - \mathbf{x}_{i} }{ h_{n} } \right) d\mathbf{x} = \int \varphi(\mathbf{u})d\mathbf{u} = 1 $
      </p>
      <p class="fragment">
      This means that as $h_{n}$ gets smaller and smaller, $\delta_{n}(\mathbf{x} - \mathbf{x}_{i})$ approaches a Dirac delta function centered at $\mathbf{x}_{i}$, and $p_{n}(\mathbf{x})$ approaches a superposition of deltas centered at the samples.
      </p>
      <p class="fragment">
      When that happens, you’ll need more and more samples to approximate the true distribution of $p(\mathbf{x})$!
      </p>
      </section>
      <section id="revisiting-the-gaussian" class="level2">
      <h2>Revisiting the Gaussian</h2>
      <figure>
      <img src="img/gaussian_windows.svg" alt="Gaussian Windows" style="width:50.0%" /><figcaption>Gaussian Windows</figcaption>
      </figure>
      </section>
      <section id="multimodal-distribution-estimation" class="level2">
      <h2>Multimodal Distribution Estimation</h2>
      <figure>
      <img src="img/shape_windows.svg" alt="Gaussian Windows" style="width:50.0%" /><figcaption>Gaussian Windows</figcaption>
      </figure>
      </section>
      <section id="classification-using-parzen-windows" class="level2">
      <h2>Classification Using Parzen Windows</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/parzen_smallbin.svg" alt="Parzen Windows: Small Bin" style="width:80.0%" /><figcaption>Parzen Windows: Small Bin</figcaption>
      </figure>
      </div>
      <div>
      <ul>
      <li>
      We can estimate the densities for each class and classify a test point by assigning it to the class with the maximum posterior.
      </li>
      <li class="fragment">
      The Parzen window-based classifier depends heavily on the form of the kernel function.
      </li>
      <li class="fragment">
      The error can be made arbitrarily low by selecting a tiny $h_{n}$, but what does this lead to?
      </li>
      </ul>
      </div>
      </div>
      </section>
      <section id="classification-using-parzen-windows-1" class="level2">
      <h2>Classification Using Parzen Windows</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/parzen_largebin.svg" alt="Parzen Windows: Large Bin" style="width:80.0%" /><figcaption>Parzen Windows: Large Bin</figcaption>
      </figure>
      </div>
      <div>
      <ul>
      <li>
      We can estimate the densities for each class and classify a test point by assigning it to the class with the maximum posterior.
      </li>
      <li>
      The Parzen window-based classifier depends heavily on the form of the kernel function.
      </li>
      <li>
      The error can be made arbitrarily low by selecting a tiny $h_{n}$, but what does this lead to?
      </li>
      </ul>
      </div>
      </div>
      </section>
      <section id="summary-of-parzen-windows" class="level2">
      <h2>Summary of Parzen Windows</h2>
      <p>If you get confused about the densities, think in terms of building a histogram.</p>
      <p class="fragment">
      You want to know how many samples fall within each range of x-values.
      </p>
      <p class="fragment">
      You have to choose the bin width parameter to properly reveal the underlying distribution.
      </p>
      <p class="fragment">
      Having more samples will lead you to a better estimation of the density; an infinite number of samples will lead to a perfect estimation (convergence).
      </p>
      <p class="fragment">
      This is a simple method, but without much else to go on, it may give you a good way of looking at your data.
      </p>
      </section>
      </section>
      <section id="section-4" class="level1">
      <h1></h1>
      <section id="k-nearest-neighbors" class="level2">
      <h2>K-Nearest Neighbors</h2>
      </section>
      <section id="second-method-for-convergence" class="level2">
      <h2>Second Method for Convergence</h2>
      <p><img src="img/convergence.svg" style="width:100.0%" /></p>
      <p><strong>K-Nearest Neighbor</strong>: Specify $k_{n}$ as a function of $n$, such as $k_{n} = \sqrt{n}$. Then, the volume $V_{n}$ is grown until it encloses $k_{n}$ neighbors of $\mathbf{x}$.</p>
      </section>
      <section id="limitation-of-parzen-windows" class="level2">
      <h2>Limitation of Parzen Windows</h2>
      <p>Selecting the window size is typically done \textit{ad hoc}.</p>
      <p class="fragment">
      Instead of picking a window size and then seeing how the data fits, why not use the training data to dictate the size of the window?
      </p>
      <p class="fragment">
      K-NN allows the window size to be a function of observed samples:
      </p>
      <ul>
      <li class="fragment">
      Center the window on $\mathbf{x}$
      </li>
      <li class="fragment">
      Let the window grow until it captures $k_{n}$ nearby samples, where $k_{n}$ is a function of $n$.
      </li>
      <li class="fragment">
      The samples within the window are the $k_{n}$ “nearest neighbors” of $\mathbf{x}$.
      </li>
      </ul>
      <p class="fragment">
      If density is high, then the window is small – high resolution.
      </p>
      <p class="fragment">
      If density is low, then the window is large (but stops at an appropriate size).
      </p>
      </section>
      <section id="k-nearest-neighbors-1" class="level2">
      <h2>K-Nearest Neighbors</h2>
      <p>$ p_{n}(\mathbf{x}) = \frac{ k_{n} }{ nV_{n} } $</p>
      <p class="fragment">
      Recall our convergence discussion from last time…
      </p>
      <p class="fragment">
      We want $\lim_{n\rightarrow\infty}{k_{n}} = \infty$, assuring that $k_{n}/n$ will estimate $p_{n}(\mathbf{x})$.
      </p>
      <p class="fragment">
      We also want $\lim_{n\rightarrow\infty}{k_{n}/n} = 0$, ensuring $k_{n}$ grows more slowly than $n$.
      </p>
      <p class="fragment">
      These conditions ensure that $p_{n}(\mathbf{x})$ converges to $p(\mathbf{x})$.
      </p>
      </section>
      <section id="examples-of-k-nn-densities" class="level2">
      <h2>Examples of K-NN Densities</h2>
      <figure>
      <img src="img/knn_hist.png" alt="K-NN Histogram" style="width:50.0%" /><figcaption>K-NN Histogram</figcaption>
      </figure>
      </section>
      <section id="examples-of-k-nn-densities-1" class="level2">
      <h2>Examples of K-NN Densities</h2>
      <figure>
      <img src="img/knn_hist3d.png" alt="K-NN Histogram" style="width:40.0%" /><figcaption>K-NN Histogram</figcaption>
      </figure>
      </section>
      <section id="k-nn-estimation-from-a-single-sample" class="level2">
      <h2>K-NN Estimation from a Single Sample</h2>
      <p>If $n=1$ and $k_{n} = \sqrt{n} = 1$, our estimate becomes:</p>
      <p class="fragment">
      $ p_{n}(\mathbf{x}) = \frac{ 1 }{ 2 |x - x_{1}|} $
      </p>
      <p class="fragment">
      This diverges (as opposed to converging) to infinity – so it’s a poor estimate of $p(\mathbf{x})$.
      </p>
      <p class="fragment">
      However, the density never reaches 0 in the finite-sample case, because instead of defining the density by some arbitrary window, we define it by the nearest possible values of the random variable (which is always nonzero).
      </p>
      <p class="fragment">
      Since $n$ never actually reaches infinity, this is an okay tradeoff in most scenarios.
      </p>
      </section>
      <section id="k-nearest-neighbor-estimates" class="level2">
      <h2>K-Nearest Neighbor Estimates</h2>
      <figure>
      <img src="img/knn_estimates.png" alt="K-NN Estimates" style="width:45.0%" /><figcaption>K-NN Estimates</figcaption>
      </figure>
      </section>
      <section id="limitations-of-k-nn" class="level2">
      <h2>Limitations of K-NN</h2>
      <p>We saw in Parzen windows we had to choose the width carefully so that we generalized well without overfitting.</p>
      <p class="fragment">
      Here, we select $k_{n}$, e.g. the number of neighbors we use to grow the region to reach.
      </p>
      <p class="fragment">
      When training is limited, $k_{n}$ can drastically alter the form of the density.
      </p>
      <p class="fragment">
      As with everything else, the choice of $k_{n}$ is done based on what gives the best results (which can be evaluated in terms of classifier accuracy).
      </p>
      </section>
      </section>
      <section id="section-5" class="level1">
      <h1></h1>
      <section id="classification-using-k-nn" class="level2">
      <h2>Classification using K-NN</h2>
      </section>
      <section id="estimation-of-a-posteriori-probabilities" class="level2">
      <h2>Estimation of A Posteriori Probabilities</h2>
      <p>We can directly estimate the posterior probabilities $P(\omega_{i} | \mathbf{x})$ from a set of $n$ labeled samples by using them to estimate the densities.</p>
      <p class="fragment">
      Suppose we place a window of volume $V$ around $\mathbf{x}$ and capture $k$ samples, $k_{i}$ of which are labeled $\omega_{i}$.
      </p>
      <p class="fragment">
      Then we estimate the joint probability $p(\mathbf{x}, \omega_{i})$ as:
      </p>
      <p class="fragment">
      $ p_{n}(\mathbf{x}, \omega_{i}) = \frac{ k_{i}/n }{ V } = \frac{ k_{i} }{ nV } $
      </p>
      <p class="fragment">
      And so we use Bayes law to get the estimate for $P(\omega_{i} | \mathbf{x} )$:
      </p>
      <p class="fragment">
      $ P_{n}( \omega_{i} | \mathbf{x} ) = \frac{ p_{n}(\mathbf{x}, \omega_{i}) }{ \sum_{j=1}^{c} p_{n}(\mathbf{x}, \omega_{j} ) } = \frac{ k_{i} }{ k } $
      </p>
      </section>
      <section id="classification-using-k-nn-1" class="level2">
      <h2>Classification Using K-NN</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/knn_classifier_01.png" alt="K-NN Classifier" style="width:70.0%" /><figcaption>K-NN Classifier</figcaption>
      </figure>
      </div>
      <div>
      <p>$ P_{n}(\omega_{i} | \mathbf{x} ) = \frac{ p_{n}(\mathbf{x}, \omega_{i}) }{ \sum_{j=1}^{c} p_{n}(\mathbf{x}, \omega_{j}) } = \frac{ k_{i} }{ k } $</p>
      <p class="fragment">
      Thus, the probability that we observe $\omega_{i}$ given $\mathbf{x}$ is simply the fraction of randomly-observed samples within the neighborhood of $\mathbf{x}$ that are labeled $\omega_{i}$.
      </p>
      </div>
      </div>
      </section>
      <section id="classification-using-k-nn-2" class="level2">
      <h2>Classification Using K-NN</h2>
      <figure>
      <img src="img/knn_classifier_voronoi.svg" alt="K-NN Voronoi" style="width:45.0%" /><figcaption>K-NN Voronoi</figcaption>
      </figure>
      </section>
      <section id="classification-using-k-nn-3" class="level2">
      <h2>Classification Using K-NN</h2>
      <figure>
      <img src="img/knn_classifier_3d.svg" alt="K-NN Voronoi in 3D" style="width:35.0%" /><figcaption>K-NN Voronoi in 3D</figcaption>
      </figure>
      </section>
      </section>
      <section id="section-6" class="level1">
      <h1></h1>
      <section id="parting-words" class="level2">
      <h2>Parting Words</h2>
      </section>
      <section id="k-nn-for-density-estimation" class="level2">
      <h2>K-NN For Density Estimation</h2>
      <p>k-NN is a conceptually simple method: it says that samples are likely to belong to the class of other samples that are nearby.</p>
      <p class="fragment">
      While technically it is a density estimation method, k-NN is often used to skip straight to performing classification.
      </p>
      <p class="fragment">
      I’m sparing you some details about convergence and proof of the error bounds, because they typically follow from the discussion about convergence with Parzen Windows.
      </p>
      <p class="fragment">
      The book has the details if you’re interested.
      </p>
      </section>
      </section>
      <section id="section-7" class="level1">
      <h1></h1>
      <section id="next-class" class="level2">
      <h2>Next Class</h2>
      </section>
      <section id="supervised-vs.-unsupervised-classification" class="level2">
      <h2>Supervised vs. Unsupervised Classification</h2>
      <p>So far, we have assumed that we knew the labels associated with our samples: $\mathcal{D}_{1}$ contains samples from $\omega_{1}$, and so on.</p>
      <p class="fragment">
      Obviously, this is not always (or even usually) the case.
      </p>
      <p class="fragment">
      If we just have a cloud of points, how do we decide how to best cluster and classify points?
      </p>
      <p class="fragment">
      The next class, we will discuss clustering, expectation maximization, and some methods for unsupervised classification.
      </p>
      </section>
      </section>
      </div>
    </div>
    <script src="js/reveal.js"></script>
    <!-- Particles scripts -->
    <script src="lib/js/particles.js"></script>
    <script src="lib/js/app.js"></script>
    <!-- Pseudocode scripts -->
    <script>
     pseudocode.renderElement(document.getElementById("hello-world-code"));
    </script>
    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
          fragments: true,
          math: {
					    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					    config: 'TeX-AMS_HTML-full'
          },

                  transition: Reveal.getQueryHash().transition || 'fade',
        
        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true },
          { src: 'plugin/math/math.js', async: true },
          { src: 'plugin/chalkboard/chalkboard.js'}
        ],
        
        // Chalkboard Plugin Settings
				chalkboard: {
					src: "plugin/chalkboard/chalkboard.json",
					toggleChalkboardButton: { left: "80px" },
					toggleNotesButton: { left: "130px" },
// 					pen:  [ 'crosshair', 'pointer' ]
//					theme: "whiteboard",
//					background: [ 'rgba(127,127,127,.1)' , 'reveal.js-plugins/chalkboard/img/whiteboard.png' ],
// 					pen:  [ 'crosshair', 'pointer' ]
//					pen: [ url('reveal.js-plugins/chalkboard/img/boardmarker.png), auto' , 'url(reveal.js-plugins/chalkboard/img/boardmarker.png), auto' ],
//				        color: [ 'rgba(0,0,255,1)', 'rgba(0,0,255,0.5)' ],
//				        draw: [ (RevealChalkboard) ?  RevealChalkboard.drawWithPen : null , (RevealChalkboard) ? RevealChalkboard.drawWithPen : null ],
				},
				keyboard: {
				    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle chalkboard when 'c' is pressed
				    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
				    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
				     8: function() { RevealChalkboard.reset() },	// reset all chalkboard data when 'BACKSPACE' is pressed
				    68: function() { RevealChalkboard.download() },	// downlad chalkboard drawing when 'd' is pressed
					  90: function() { Recorder.downloadZip(); }, 	// press 'z' to download zip containing audio files
					  84: function() { Recorder.fetchTTS(); } 	// press 't' to fetch TTS audio files
				},
      });

    </script>
  </body>
</html>

<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Convolutional Neural Networks <br/>(Pt. 2)</title>

    <meta name="description" content="Convolutional Neural Networks <br/>(Pt. 2)">    

        <meta name="author" content="Scott Doyle" />
    
    <link rel="stylesheet" href="css/reset.css">
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- Custom Addons: TikzJax -->
    <link rel="stylesheet" type="text/css" href="http://tikzjax.com/v1/fonts.css">
    <script src="http://tikzjax.com/v1/tikzjax.js"></script>
    <!-- Custom Addons: pseudocode.js -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
    <!-- Set Theme -->
        <link rel="stylesheet" href="css/theme/scottdoy.css" id="theme">
    
    <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/atelier-dune-light.css">
    

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

          </head>

  <body>

  
  <div class="reveal">
    <div class="slides">
      <!-- Custom Title Section Here -->
      <section data-background="#005bbb" id="particles-js" class="level1">
        <section id="title" class="level2">
        <h1 style="color: #e4e4e4;">Convolutional Neural Networks <br/>(Pt. 2)</h1>
        <p>
        <h3 style="color: #e4e4e4;">Machine Learning for Biomedical Data</h2>
        </p>
        <p style="color: #e4e4e4;"><small>Scott Doyle / scottdoy@buffalo.edu</small></p>
        </section>
      </section>

      <!-- Custom TOC Section here-->
      
      <!-- Insert Body -->
      <section id="section" class="level1">
      <h1></h1>
      <section id="recap-last-lecture" class="level2">
      <h2>Recap Last Lecture</h2>
      </section>
      <section id="recap-rationale-for-deep-networks" class="level2">
      <h2>Recap: Rationale for Deep Networks</h2>
      <p>In theory, neural networks can replicate any function (decision surface), no matter how complicated. “In theory”.</p>
      <p class="fragment">
      In reality, this would require an unreasonable number of:
      </p>
      <ul>
      <li class="fragment">
      <strong>Input nodes</strong>, to describe increasingly large data types;
      </li>
      <li class="fragment">
      <strong>Input samples</strong>, to accurately describe a complex, varied class space;
      </li>
      <li class="fragment">
      <strong>Hidden nodes</strong>, to capture the nonlinear relationship between the inputs and desired outputs;
      </li>
      <li class="fragment">
      <strong>Hours</strong>, to fully train a network on all this data;
      </li>
      <li class="fragment">
      <strong>Dollars</strong>, to buy the hardware required for such a massive undertaking.
      </li>
      </ul>
      <p class="fragment">
      Surely, there must be a better way?
      </p>
      </section>
      <section id="recap-deep-learning" class="level2">
      <h2>Recap: Deep Learning</h2>
      <p>Deep learning, the stacking of several hidden layers together, is enabled by a few insights:</p>
      <ul>
      <li class="fragment">
      You can design the network such that it is <strong>differentiable end-to-end</strong>; in other words, you can train each layer with gradient descent via backpropagation, and learn an optimal set of weights.
      </li>
      <li class="fragment">
      You don’t need to have <strong>fully connected layers</strong> – you can replace those layers with other, sparser layers that look at a limited input space, or certain “aspects” of the data, and learn weights that tie those regions together.
      </li>
      <li class="fragment">
      Nowadays, you have <strong>ridiculously large datasets</strong> from which you can pull training data. This will expose your network to sufficient amount of variability across a very complex class space while avoiding overfitting.
      </li>
      </ul>
      </section>
      <section id="recap-schematic-of-fully-connected-networks" class="level2">
      <h2>Recap: Schematic of Fully-Connected Networks</h2>
      <figure>
      <img src="img/neural_net_schematic.jpeg" style="width:90.0%" alt="" /><figcaption>Fully-Connected Neural Network</figcaption>
      </figure>
      </section>
      <section id="recap-scalability-of-traditional-neural-nets" class="level2">
      <h2>Recap: Scalability of Traditional Neural Nets</h2>
      <p>As we increase pixel sizes, this number of inputs balloons very quickly: a 200x200 color image would result in 120,000 weights, and so on.</p>
      <p class="fragment">
      Moreover, each of these must be mapped to hidden units – so our weight vector is 120,000 multiplied by some (likely larger) number of hidden units.
      </p>
      <p class="fragment">
      And that’s all for one layer, and for a dataset of just 200x200x3 images, which are quite small.
      </p>
      </section>
      <section id="recap-cnn-rationale" class="level2">
      <h2>Recap: CNN Rationale</h2>
      <p><strong>Convolutional Neural Networks</strong> are specifically-designed to operate on image data. This enables a few assumptions and tricks:</p>
      <ul>
      <li class="fragment">
      Image inputs are <strong>geographically important</strong>, meaning that image features from one part of the image don’t have to be connected to those in another part of the image.
      </li>
      <li class="fragment">
      In the case of color images (3D), hidden nodes can be arranged as a <strong>3D volume</strong> instead of a single “layer”, allowing the same part of the image to be analyzed in different ways (i.e. using different filter parameters).
      </li>
      <li class="fragment">
      As the data is processed, we can reduce the number of nodes at each layer by downsampling the image (pooling). We can still recognize the image content, even as it is reduced to a very small size.
      </li>
      </ul>
      </section>
      <section id="recap-schematic-of-convolutional-nets" class="level2">
      <h2>Recap: Schematic of Convolutional Nets</h2>
      <figure>
      <img src="img/cnn_schematic.jpeg" style="width:100.0%" alt="" /><figcaption>Convolutional Neural Network</figcaption>
      </figure>
      </section>
      <section id="recap-cnn-layers" class="level2">
      <h2>Recap: CNN Layers</h2>
      <p>There are several types of layers in CNNs which process the input data:</p>
      <ul>
      <li class="fragment">
      <strong>Convolutional Layers</strong>: A kernel is convolved with the input. The height and width of the output is a function of the filter parameters (padding, kernel size, and stride), while the depth of the output is a hyperparameter of the layer.
      </li>
      <li class="fragment">
      <strong>Rectified Linear Units</strong>: A type of activation function, ReLU layers take the form of $\max(0,x)$. This prevents gradients from getting unmanagable (exploding or vanishing).
      </li>
      <li class="fragment">
      <strong>Pooling Layers</strong>: Downsamples the input volume in the height and width dimensions. This reduces the number of neurons / parameters that must be learned, without reducing the expressive power of the network.
      </li>
      <li class="fragment">
      <strong>Fully-Connected Layers</strong>: Same type of layer as in traditional nets, where each neuron is fully connected to each element of the input.
      </li>
      </ul>
      </section>
      <section id="recap-example-of-convolutional-nets-operating-on-an-image" class="level2">
      <h2>Recap: Example of Convolutional Nets Operating on an Image</h2>
      <p><img src="img/convnet_overview.jpeg" style="width:70.0%" /></p>
      </section>
      </section>
      <section id="section-1" class="level1">
      <h1></h1>
      <section id="practical-concerns-and-design-hints" class="level2">
      <h2>Practical Concerns and Design Hints</h2>
      </section>
      <section id="tips-and-tricks" class="level2">
      <h2>Tips and Tricks</h2>
      <p>What follows are some general rules of thumb to get you started using CNNs.</p>
      <p class="fragment">
      Feel free to experiment with alternate approaches! These are not set in stone.
      </p>
      </section>
      <section id="tip-fewer-smaller-conv-layers" class="level2">
      <h2>Tip: Fewer, Smaller CONV Layers</h2>
      <p>If you have three 3x3 [CONV] layers on each other, the third layer will have a 7x7 “view” of the input volume. Compare that with a single [CONV] layer with a 7x7 receptive field. <strong>The three 3x3 [CONV] layers are better.</strong></p>
      <ul>
      <li class="fragment">
      The three [CONV] layers will have non-linearities sandwiched in-between them, which the 7x7 [CONV] layer won’t have. This gives them more expressive power.
      </li>
      <li class="fragment">
      If all volumes have $C$ channels, then the 7x7 [CONV] layer contains $(C \times (7 \times 7 \times C) = 49C^{2}$ parameters, but the three 3x3 layers will have $3 \times (C \times (3 \times 3 \times C)) = 27C^{2}$ parameters.
      </li>
      </ul>
      </section>
      <section id="tip-when-in-doubt-steal" class="level2">
      <h2>Tip: When in Doubt, Steal!</h2>
      <p>Every month, people are testing their architectures against some benchmark datasets like ImageNet.</p>
      <p class="fragment">
      <strong>Unless you are interested specifically in deep learning architecture design</strong>, most of these small, incremental tweaks should not be of central interest to you.
      </p>
      <ul>
      <li class="fragment">
      If you’re using SVMs to determine outcome in patient data, you wouldn’t try to write a paper about a new type of SVM kernel function.
      </li>
      <li class="fragment">
      If you are writing about a new type of genomic pathway, you don’t also need to discuss a small change to your PCR protocol.
      </li>
      <li class="fragment">
      Similarly, if you’re trying to identify a tumor region in an image of tissue, you don’t need to also invent a new type of nonlinearity or convolutional layer.
      </li>
      </ul>
      </section>
      <section id="tip-when-in-doubt-steal-1" class="level2">
      <h2>Tip: When in Doubt, Steal!</h2>
      <p>So if that’s the case, what architecture should you choose?</p>
      <ul>
      <li class="fragment">
      Use whatever is available for your software stack;
      </li>
      <li class="fragment">
      Use whatever has worked for other researchers looking at the same or similar problems;
      </li>
      <li class="fragment">
      Use whatever is currently working the best on benchmark datasets.
      </li>
      </ul>
      <p class="fragment">
      While your application may not be able to use the same weights, at least the architecture will be taken care of.
      </p>
      </section>
      <section id="hyperparameter-adjustment" class="level2">
      <h2>Hyperparameter Adjustment</h2>
      <p>Okay, so we can pick a certain order of layer stacking. We have a pile of images sitting on the hard drive, and we have labels associated with each image.</p>
      <p class="fragment">
      So now we need to format our data and set up our layer <strong>hyperparameters</strong>: that is, the sizes and operations at each layer, which are NOT learned by gradient descent.
      </p>
      <p class="fragment">
      Here are some guidelines for adjustments.
      </p>
      </section>
      <section id="input-layer" class="level2">
      <h2>Input Layer</h2>
      <p>Your input layer – i.e. the image dimensions – should be divisible by 2.</p>
      <p class="fragment">
      You’ll typically see 32, 64, 96, 224, 384, and 512.
      </p>
      <p class="fragment">
      <strong>Why these sizes?</strong> That’s a great question!
      </p>
      <ul>
      <li class="fragment">
      Even numbers help to make the convolutional math work out.
      </li>
      <li class="fragment">
      Smaller images require less memory, so older / simpler nets typically use smaller sizes.
      </li>
      <li class="fragment">
      Larger images are used to hold more complex scenes / classes, within reason.
      </li>
      <li class="fragment">
      More inputs require more nodes for processing, which increases dataset size requirements and training time limits.
      </li>
      </ul>
      </section>
      <section id="convolutional-layers" class="level2">
      <h2>Convolutional Layers</h2>
      <p>Layers should use small, odd-numbered filter sizes ($\mathbf{F}=3$, $\mathbf{F}=5$), with a stride of $\mathbf{S}=1$, and padding $\mathbf{P}$ should be used so that the convolutional layer does not alter the spatial size of the output. Recall that for any $\mathbf{F}$:</p>
      <p class="fragment">
      $ \mathbf{P} = \frac{(\mathbf{F} - 1)}{2}$
      </p>
      <p class="fragment">
      will preserve input size.
      </p>
      <p class="fragment">
      Larger filters, e.g. $\mathbf{F}=7$, are only seen on the first convolutional layer, if at all.
      </p>
      </section>
      <section id="pooling-layers" class="level2">
      <h2>Pooling Layers</h2>
      <p>By far the most common pooling layer performs <strong>max-pooling</strong> with receptive field $\mathbf{F}=2$ and stride $\mathbf{S}=2$. Increasing these numbers means that you are aggressively downsampling your data, which leads to too much information loss.</p>
      </section>
      <section id="sizing-concerns" class="level2">
      <h2>Sizing Concerns</h2>
      <p>This approach means that CONV layers always preserve input sizes, meaning that they are just responsible for learning image features (and not image downsizing).</p>
      <p class="fragment">
      The POOL layers are the only ones concerned with downsampling the image.
      </p>
      <p class="fragment">
      If you don’t do this – if you use $\mathbf{S}&gt;1$ or $\mathbf{P}=0$ – you have to keep track of the volume as it changes size throughout the network.
      </p>
      <p class="fragment">
      Depending on the software you’re using, if you make a mistake, your system will likely give you an error when it tries to run the network.
      </p>
      </section>
      <section id="questions-and-answers" class="level2">
      <h2>Questions and Answers</h2>
      <p><strong>Stride of 1 in CONV</strong>: Smaller strides tend to work better in practice, and prevent CONV layers from downsampling.</p>
      <p class="fragment">
      <strong>Padding</strong>: Padding keeps the volumes from changing as a result of the convolution operation, but also allows you to “retain” information at the boundary instead of seeing it reduced at each convolutional pass.
      </p>
      <p class="fragment">
      <strong>Memory</strong>: GPUs are great for performing backpropagation calculations, but have relatively little onboard memory. Filtering a $224\times224\times3$ image with a typical architecture can lead to several millions of activations per image. If you have to compromise, do it at the beginning in the first CONV layer by using larger filter sizes and strides.
      </p>
      </section>
      <section id="calculating-memory-requirements" class="level2">
      <h2>Calculating Memory Requirements</h2>
      <p>Memory in CNNs is taken up by three sources:</p>
      <ul>
      <li class="fragment">
      <strong>Activations</strong>: Intermediate volume sizes have a number of activations plus an equal number of gradients to keep track of.
      </li>
      <li class="fragment">
      <strong>Parameters</strong>: These numbers hold the network parameters, the gradients during backprop, and also a step size / learning rate.
      </li>
      <li class="fragment">
      <strong>Miscellaneous</strong>: Image batch sizes, augmented images / parameters, etc.
      </li>
      </ul>
      <p class="fragment">
      You can get a rough estimate of these values, multiply by 4 to get the number of bytes needed, then divide by 1024 to get KB, MB, and GB. Then compare that to your GPU’s memory size (typically 4, 6, or 12 if you’ve got good hardware).
      </p>
      <p class="fragment">
      To reduce memory, the first thing you should look at is reducing the batch size, which in turn reduces your activations.
      </p>
      </section>
      </section>
      <section id="section-2" class="level1">
      <h1></h1>
      <section id="design-of-convolutional-network-architectures" class="level2">
      <h2>Design of Convolutional Network Architectures</h2>
      </section>
      <section id="standard-layer-configurations" class="level2">
      <h2>Standard Layer Configurations</h2>
      <p>The order and number of layers in a CNN (CONV, POOL, RELU, FC) defines the <strong>architecture</strong> of the network. Some famous architectures have been given names by the folks who designed them and proved their effectiveness over competing architectures.</p>
      <p class="fragment">
      How does the architecture affect the processing of the network?
      </p>
      </section>
      <section id="effect-of-architecture-on-processing" class="level2">
      <h2>Effect of Architecture on Processing</h2>
      <ul>
      <li><p><small>$\left[INPUT\right] \rightarrow \left[FC\right]$</small></p>
      <p>Regular linear classifier (linear discriminant)</p></li>
      <li><p><small>$\left[INPUT\right] \rightarrow \left[CONV\right] \rightarrow \left[RELU\right] \rightarrow \left[FC\right]$</small></p>
      <p>Most basic kind of convolutional network</p></li>
      <li><p><small>$\left[\textrm{INPUT}\right] \rightarrow \left[\left[CONV\right] \rightarrow \left[RELU\right] \rightarrow \left[POOL\right]\right]*2 \rightarrow \left[FC\right] \rightarrow \left[RELU\right] \rightarrow \left[FC\right]$</small></p>
      <p>Here there is a convolutional layer paired with each pooling layer, followed by a fully-connected “head” section.</p></li>
      <li><p><small>$\left[INPUT\right] \rightarrow \left[\left[\left[CONV\right] \rightarrow \left[RELU\right]\right]*2 \rightarrow POOL\right]*3 \rightarrow \ \left[\left[FC\right] \rightarrow \left[RELU\right]\right]*2 \rightarrow\left[FC\right] $</small></p>
      <p>By allowing multiple convolutions to take place before the destructive pooling operation, we can ensure that deeper, more informative features get learned.</p></li>
      </ul>
      </section>
      </section>
      <section id="section-3" class="level1">
      <h1></h1>
      <section id="specific-cnn-architectures" class="level2">
      <h2>Specific CNN Architectures</h2>
      </section>
      <section id="lenet" class="level2">
      <h2>LeNet</h2>
      <p>Developed in the 1990’s, LeNet was used for handwriting digit analysis on the MNIST dataset.</p>
      </section>
      <section id="lenet-diagram" class="level2">
      <h2>LeNet: Diagram</h2>
      <figure>
      <img src="img/lenet.svg" style="width:100.0%" alt="" /><figcaption>LeNet Schematic</figcaption>
      </figure>
      </section>
      <section id="alexnet" class="level2">
      <h2>AlexNet</h2>
      <p>Won the ImageNet ILSVRC 2012 challenge; larger and deeper than LeNet, with multiple stacked CONV layers.</p>
      </section>
      <section id="alexnet-diagram" class="level2">
      <h2>AlexNet: Diagram</h2>
      <figure>
      <img src="img/alexnet.svg" style="width:100.0%" alt="" /><figcaption>AlexNet Schematic (Cut off in original paper)</figcaption>
      </figure>
      </section>
      <section id="googlenet" class="level2">
      <h2>GoogLeNet</h2>
      <p>GoogLeNet won the ILSVRC 2014 challenge. The team developed an “inception” module, which reduced the number of parameters in the network from 60M (in AlexNet) to 4M.</p>
      <p>They also replaced the FC layers with “average pooling” layers.</p>
      </section>
      <section id="googlenet-inception-module" class="level2">
      <h2>GoogLeNet: Inception module</h2>
      <figure>
      <img src="img/googlenet_inception.svg" style="width:100.0%" alt="" /><figcaption>Inception module</figcaption>
      </figure>
      </section>
      <section id="googlenet-diagram" class="level2">
      <h2>GoogLeNet: Diagram</h2>
      <figure>
      <img src="img/googlenet.svg" style="width:10.0%" alt="" /><figcaption>GoogLeNet Diagram</figcaption>
      </figure>
      </section>
      <section id="vggnet" class="level2">
      <h2>VGGNet</h2>
      <p>The second-place team in ILSVRC 2014 was the VGGNet, which showed that the depth of the network is a critical component of performance. This network uses a lot more parameters (140M), and therefore memory and computation, but later iterations removed some of these parameters.</p>
      </section>
      <section id="vggnet-diagram" class="level2">
      <h2>VGGNet: Diagram</h2>
      <figure>
      <img src="img/vgg_config.svg" style="width:40.0%" alt="" /><figcaption>VGG Configuration</figcaption>
      </figure>
      </section>
      <section id="resnet" class="level2">
      <h2>ResNet</h2>
      <p>Deeper nets seem to work better, so won’t we get increasing performance if we just keep slamming layers together?</p>
      <p class="fragment">
      It turns out that each time a gradient is “propagated” down through a very long set of layers, the value of the gradient decreases. This is called the <strong>vanishing gradient</strong> problem.
      </p>
      <p class="fragment">
      To solve this, ResNet uses <strong>skip connections</strong>: In a normal network, the activation at a layer is $y=f(x)$, where $f(x)$ is our nonlinear function that is differentiated during backpropagation. In a skip connection, this is redefined as: $y=f(x) + x$, which allows the gradient to be preserved as it travels back through the network.
      </p>
      </section>
      <section id="resnet-skip-connections" class="level2">
      <h2>ResNet: Skip Connections</h2>
      <figure>
      <img src="img/resnet_skip.svg" style="width:60.0%" alt="" /><figcaption>Skip Connection Diagram</figcaption>
      </figure>
      </section>
      <section id="resnet-diagram" class="level2">
      <h2>ResNet: Diagram</h2>
      <figure>
      <img src="img/resnet.svg" style="width:20.0%" alt="" /><figcaption>ResNet</figcaption>
      </figure>
      </section>
      </section>
      <section id="section-4" class="level1">
      <h1></h1>
      <section id="what-does-a-cnn-see" class="level2">
      <h2>What does a CNN “See”?</h2>
      </section>
      <section id="so-whats-it-doing" class="level2">
      <h2>So… What’s it… Doing?</h2>
      <p>A huge elephant in the room when discussing CNNs is: What is actually going on at each of these layers? How does it actually work?</p>
      <p class="fragment">
      To answer this, folks have tried to look at what’s going on at different points in the “process”.
      </p>
      <p class="fragment">
      For example, Mahendran and Vedaldi (2014) showed that you can “invert” the representations of the image at layers in the CNN by using a natural image as a prior and then presenting random noise to the network. Then you look at the outputs of the layers.
      </p>
      </section>
      <section id="example-inverted-images" class="level2">
      <h2>Example Inverted Images</h2>
      <figure>
      <img src="img/inversion_orig.svg" style="width:100.0%" alt="" /><figcaption>Original Images</figcaption>
      </figure>
      </section>
      <section id="example-inverted-images-1" class="level2">
      <h2>Example Inverted Images</h2>
      <figure>
      <img src="img/inversion_reconstruction.svg" style="width:100.0%" alt="" /><figcaption>“Inverted” Images</figcaption>
      </figure>
      </section>
      <section id="intermediate-layer-representations" class="level2">
      <h2>Intermediate Layer Representations</h2>
      <div class="l-double">
      <div>
      <figure>
      <img src="img/orig_tissue.svg" style="width:80.0%" alt="" /><figcaption>Original Image</figcaption>
      </figure>
      </div>
      <div>
      <figure>
      <img src="img/DL_result.svg" style="width:80.0%" alt="" /><figcaption>Tissue Map from Deep Learning</figcaption>
      </figure>
      </div>
      </div>
      </section>
      <section id="alexnet-architecture" class="level2">
      <h2>AlexNet Architecture</h2>
      <figure>
      <img src="img/alexnet_caffe.svg" style="width:10.0%" alt="" /><figcaption>Modified AlexNet Setup</figcaption>
      </figure>
      </section>
      <section id="intermediate-layer-activations" class="level2">
      <h2>Intermediate Layer Activations</h2>
      <div class="l-double">
      <div>
      <p><img src="img/activation01.svg" style="width:45.0%" alt="Activation Layer 01" /> <img src="img/activation03.svg" style="width:45.0%" alt="Activation Layer 03" /></p>
      </div>
      <div>
      <p><img src="img/activation02.svg" style="width:45.0%" alt="Activation Layer 01" /> <img src="img/activation04.svg" style="width:45.0%" alt="Activation Layer 03" /></p>
      </div>
      </div>
      </section>
      <section id="artistic-style-vs.-content" class="level2">
      <h2>Artistic Style vs. Content</h2>
      <p>So with that understanding, Gatys, et al. (2015) realized that you can separate the “content” of the image (represented at deeper layers as objects and locations) from the “pixel values” or “style” of the image (represented at shallow layers, right after the first convolutional layers).</p>
      <p class="fragment">
      Essentially you can pull the textures from one image, the content from another, and mix them!
      </p>
      </section>
      <section id="artistic-style-framework" class="level2">
      <h2>Artistic Style Framework</h2>
      <figure>
      <img src="img/artistic_diagram.png" style="width:60.0%" alt="" /><figcaption>Content Versus Style</figcaption>
      </figure>
      </section>
      <section id="artistic-style-examples" class="level2">
      <h2>Artistic Style Examples</h2>
      <figure>
      <img src="img/artistic_style_orig.png" style="width:60.0%" alt="" /><figcaption>Neckarfront in T{"u}bingen, Germany (Photo: Andreas Praefcke)</figcaption>
      </figure>
      </section>
      <section id="artistic-style-examples-1" class="level2">
      <h2>Artistic Style Examples</h2>
      <figure>
      <img src="img/artistic_style_01.png" style="width:60.0%" alt="" /><figcaption>\textit{The Shipwreck of the Minotaur} by J.M.W. Turner, 1805</figcaption>
      </figure>
      </section>
      <section id="artistic-style-examples-2" class="level2">
      <h2>Artistic Style Examples</h2>
      <figure>
      <img src="img/artistic_style_02.png" style="width:60.0%" alt="" /><figcaption>\textit{The Starry Night} by Vincent van Gogh, 1889</figcaption>
      </figure>
      </section>
      <section id="artistic-style-examples-3" class="level2">
      <h2>Artistic Style Examples</h2>
      <figure>
      <img src="img/artistic_style_03.png" style="width:60.0%" alt="" /><figcaption>\textit{Der Schrei} by Edvard Munch, 1893</figcaption>
      </figure>
      </section>
      <section id="artistic-style-examples-4" class="level2">
      <h2>Artistic Style Examples</h2>
      <figure>
      <img src="img/artistic_style_04.png" style="width:60.0%" alt="" /><figcaption>\textit{Femme nue assise} by Pablo Picasso, 1910</figcaption>
      </figure>
      </section>
      <section id="artistic-style-examples-5" class="level2">
      <h2>Artistic Style Examples</h2>
      <figure>
      <img src="img/artistic_style_05.png" style="width:60.0%" alt="" /><figcaption>\textit{Composition VII} by Wassily Kandinsky, 1913</figcaption>
      </figure>
      </section>
      <section id="artistic-style-at-home" class="level2">
      <h2>Artistic Style At Home!</h2>
      <p>One of the great things about recent research is the availability of open source tools for implementing these techniques.</p>
      <p class="fragment">
      With just a bit of set-up, you can implement these techniques on your own datasets. For example, check out <strong><a href="https://github.com/ebenolson/pydata15">https://github.com/ebenolson/pydata2015</a></strong>.
      </p>
      </section>
      <section id="kyoshi" class="level2">
      <h2>Kyoshi!</h2>
      <figure>
      <img src="img/kyoshi_sm.png" style="width:40.0%" alt="" /><figcaption>Kyoshi, A Good Girl</figcaption>
      </figure>
      </section>
      <section id="artistic-kyoshi" class="level2">
      <h2>Artistic Kyoshi!</h2>
      <figure>
      <img src="img/kyoshi_artstyle.png" style="width:50.0%" alt="" /><figcaption>Application of Artistic Styles</figcaption>
      </figure>
      </section>
      <section id="deep-dreaming-and-beyond" class="level2">
      <h2>Deep Dreaming and Beyond</h2>
      <p>Finally, you can do some really crazy stuff when you start looking at these networks in detail and examining / manipulating their outputs…</p>
      <p><a href="http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html">Google’s Deep Dream (Pt. 1)</a></p>
      <p><a href="http://googleresearch.blogspot.com/2015/07/deepdream-code-example-for-visualizing.html">Google’s Deep Dream (Pt. 2)</a></p>
      </section>
      </section>
      <section id="section-5" class="level1">
      <h1></h1>
      <section id="next-class" class="level2">
      <h2>Next Class</h2>
      </section>
      <section id="keep-reading" class="level2">
      <h2>Keep Reading!</h2>
      <p>CNNs are extremely popular, and there are new architectures and tweaks coming out all the time.</p>
      <p class="fragment">
      Keep an eye on developments in this field, but remember to <strong>keep it simple</strong>!
      </p>
      <p class="fragment">
      Don’t use a fancy new classifier just because you can, use it if you benefit from it!
      </p>
      </section>
      <section id="variational-autoencoders-and-generative-networks" class="level2">
      <h2>Variational Autoencoders and Generative Networks</h2>
      <p>There are two other CNN architectures I’d like to cover:</p>
      <ul>
      <li class="fragment">
      <strong>Variational Autoencoders:</strong> These learn image structure from unlabeled samples, providing a way to do dimensionality reduction and comparison in an “image space” defined by a CNN.
      </li>
      <li class="fragment">
      <strong>Generative Adversarial Networks:</strong> These networks are similar to DeepDream, where they are able to create images based on their “understanding” of the image space.
      </li>
      </ul>
      <p class="fragment">
      After this we will move on to non-image sequence datasets using <strong>Recurrent Neural Networks (RNNs)</strong>.
      </p>
      </section>
      </section>
      </div>
    </div>
    <script src="js/reveal.js"></script>
    <!-- Particles scripts -->
    <script src="lib/js/particles.js"></script>
    <script src="lib/js/app.js"></script>
    <!-- Pseudocode scripts -->
    <script>
     pseudocode.renderElement(document.getElementById("hello-world-code"));
    </script>
    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
          fragments: true,
          math: {
					    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					    config: 'TeX-AMS_HTML-full'
          },

                  transition: Reveal.getQueryHash().transition || 'fade',
        
        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true },
          { src: 'plugin/math/math.js', async: true },
          { src: 'plugin/chalkboard/chalkboard.js'}
        ],
        
        // Chalkboard Plugin Settings
				chalkboard: {
					src: "plugin/chalkboard/chalkboard.json",
					toggleChalkboardButton: { left: "80px" },
					toggleNotesButton: { left: "130px" },
// 					pen:  [ 'crosshair', 'pointer' ]
//					theme: "whiteboard",
//					background: [ 'rgba(127,127,127,.1)' , 'reveal.js-plugins/chalkboard/img/whiteboard.png' ],
// 					pen:  [ 'crosshair', 'pointer' ]
//					pen: [ url('reveal.js-plugins/chalkboard/img/boardmarker.png), auto' , 'url(reveal.js-plugins/chalkboard/img/boardmarker.png), auto' ],
//				        color: [ 'rgba(0,0,255,1)', 'rgba(0,0,255,0.5)' ],
//				        draw: [ (RevealChalkboard) ?  RevealChalkboard.drawWithPen : null , (RevealChalkboard) ? RevealChalkboard.drawWithPen : null ],
				},
				keyboard: {
				    67: function() { RevealChalkboard.toggleNotesCanvas() },	// toggle chalkboard when 'c' is pressed
				    66: function() { RevealChalkboard.toggleChalkboard() },	// toggle chalkboard when 'b' is pressed
				    46: function() { RevealChalkboard.clear() },	// clear chalkboard when 'DEL' is pressed
				     8: function() { RevealChalkboard.reset() },	// reset all chalkboard data when 'BACKSPACE' is pressed
				    68: function() { RevealChalkboard.download() },	// downlad chalkboard drawing when 'd' is pressed
					  90: function() { Recorder.downloadZip(); }, 	// press 'z' to download zip containing audio files
					  84: function() { Recorder.fetchTTS(); } 	// press 't' to fetch TTS audio files
				},
      });

    </script>
  </body>
</html>
